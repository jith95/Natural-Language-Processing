{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SREEJITH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, SimpleRNN, LSTM, TimeDistributed\n",
    "nltk.download('punkt')\n",
    "\n",
    "#opened the donald trump speech file\n",
    "speeches= open(r\"speeches.txt\", encoding=\"utf8\")\n",
    "speeches = speeches.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used the sentence tokenizer and converted everything to lowercase and then removed \"\\n\", \" \\' \" and unnecessary punctuations.\n",
    "cleanedsentences=[]\n",
    "tokenizedsentences = nltk.sent_tokenize(speeches)\n",
    "for i in tokenizedsentences:\n",
    "    i=i.lower()\n",
    "    cleansentence=re.sub(\"\\n\", ' ', i)\n",
    "    cleansentence=re.sub(\"\\’\", '', i)\n",
    "    cleansentence= \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in cleansentence]).split())\n",
    "    cleanedsentences.append(cleansentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing the dataset into 80% train and 20% test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = cleanedsentences[:int(len(cleanedsentences)*0.8)]\n",
    "test_data = cleanedsentences[int(len(cleanedsentences)*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)Classical N gram approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general ngram function with n>=1, printperplexity decides whether the perplexity should be printed(1) or not(0)\n",
    "def ngram(n,printperplexity):\n",
    "    ngram_dict = {}\n",
    "    total_ngrams = 0\n",
    "    prod=1\n",
    "    uniquewords={}\n",
    "    uniquewordcount=0\n",
    "    for sentence in train_data:\n",
    "      words = nltk.word_tokenize(sentence)\n",
    "      for word in words:\n",
    "            if word not in uniquewords:\n",
    "                uniquewordcount+=1\n",
    "      ngram_list = ngrams(words, n)\n",
    "      for ngram_combination in ngram_list:\n",
    "        total_ngrams += 1\n",
    "        if ngram_combination in ngram_dict:\n",
    "            ngram_dict[ngram_combination] += 1\n",
    "        else:\n",
    "            ngram_dict[ngram_combination] = 1\n",
    "    ngram_prob_dict = {}\n",
    "    for key, val in reversed(sorted(ngram_dict.items(), key = operator.itemgetter(1))):\n",
    "      ngram_prob_dict[str(key)] = val / total_ngrams\n",
    "      prod=prod*(val**(1/len(ngram_dict)))\n",
    "    perplexity=product/total_ngrams\n",
    "    if(printperplexity==0):\n",
    "        print(\"\\nTotal possible ngrams=\")\n",
    "        print(uniquewordcount**n)\n",
    "        print(\"\\n Ngrams that actually exist=\")\n",
    "        print(total_ngrams)\n",
    "        print(\"\\n MLE for top 50 :\\n\")\n",
    "        print(list(ngram_prob_dict.items())[:50])\n",
    "    elif(printperplexity==1):    \n",
    "        print(\"Perplexity:\")\n",
    "        print(perplexity)    \n",
    "    else:\n",
    "        return total_ngrams,ngram_prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) maximum likelihood estimates of each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram model\n",
      "\n",
      "Total possible ngrams=\n",
      "135786\n",
      "\n",
      " Ngrams that actually exist=\n",
      "135786\n",
      "\n",
      " MLE for top 50 :\n",
      "\n",
      "[(\"('the',)\", 0.03439235267258775), (\"('to',)\", 0.03255858483201508), (\"('and',)\", 0.03210198400424197), (\"('i',)\", 0.02859646797166129), (\"('a',)\", 0.02132767737469253), (\"('you',)\", 0.01840395917104856), (\"('of',)\", 0.01711516651201155), (\"('we',)\", 0.015892654618296437), (\"('it',)\", 0.014353467956932231), (\"('that',)\", 0.013653837656312139), (\"('have',)\", 0.013602285949950658), (\"('they',)\", 0.012821645825048238), (\"('going',)\", 0.012320857820393855), (\"('in',)\", 0.011864256992620741), (\"('so',)\", 0.009109922967021638), (\"('is',)\", 0.009036277672219522), (\"('but',)\", 0.00829982472419837), (\"('know',)\", 0.007916869191227372), (\"('were',)\", 0.007835859366945046), (\"('people',)\", 0.0077622140721429304), (\"('–',)\", 0.007526549128776163), (\"('its',)\", 0.00741608118657299), (\"('be',)\", 0.007150958125285376), (\"('are',)\", 0.006318766294021475), (\"('for',)\", 0.006208298351818302), (\"('not',)\", 0.006038914173773438), (\"('this',)\", 0.00600209152637238), (\"('do',)\", 0.005965268878971323), (\"('our',)\", 0.005891623584169207), (\"('with',)\", 0.005781155641966035), (\"('was',)\", 0.0057590620535254), (\"('he',)\", 0.005744332994564977), (\"('dont',)\", 0.0056265005228815935), (\"('all',)\", 0.005044702693944884), (\"('im',)\", 0.004912141163301077), (\"('what',)\", 0.004860589456939596), (\"('because',)\", 0.004632289043053039), (\"('want',)\", 0.004521821100849866), (\"('very',)\", 0.004514456571369656), (\"('me',)\", 0.004411353158646694), (\"('said',)\", 0.004396624099686271), (\"('theyre',)\", 0.004374530511245637), (\"('on',)\", 0.004315614275403944), (\"('about',)\", 0.003800097211789139), (\"('get',)\", 0.003800097211789139), (\"('great',)\", 0.003800097211789139), (\"('think',)\", 0.0037264519169870236), (\"('like',)\", 0.003719087387506812), (\"('if',)\", 0.0036233485042640626), (\"('one',)\", 0.003431870737778563)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigram model\")\n",
    "ngram(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEJITH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total possible ngrams=\n",
      "18437837796\n",
      "\n",
      " Ngrams that actually exist=\n",
      "122596\n",
      "\n",
      " MLE for top 50 :\n",
      "\n",
      "[(\"('going', 'to')\", 0.012341348820516167), (\"('you', 'know')\", 0.005522203008254755), (\"('we', 'have')\", 0.004510750758589187), (\"('were', 'going')\", 0.004274201442135143), (\"('and', 'i')\", 0.00391529903096349), (\"('to', 'be')\", 0.003907142157982316), (\"('of', 'the')\", 0.0038255734281705763), (\"('want', 'to')\", 0.0032790629384319226), (\"('have', 'to')\", 0.0030425136219778787), (\"('to', 'do')\", 0.0026428268459003554), (\"('in', 'the')\", 0.0025857287350321383), (\"('i', 'dont')\", 0.0025775718620509643), (\"('a', 'lot')\", 0.002479689386276877), (\"('i', 'think')\", 0.0022757675617475284), (\"('have', 'a')\", 0.0021534144670299193), (\"('i', 'mean')\", 0.0021452575940487453), (\"('and', 'they')\", 0.002055531991255832), (\"('i', 'have')\", 0.001990277007406441), (\"('i', 'said')\", 0.001941335769519397), (\"('lot', 'of')\", 0.0018679239126888317), (\"('and', 'we')\", 0.0017129433260465268), (\"('and', 'you')\", 0.0016232177232536134), (\"('by', 'the')\", 0.0015090215015171784), (\"('to', 'have')\", 0.0014763940095924826), (\"('not', 'going')\", 0.0014600802636301348), (\"('look', 'at')\", 0.0014437665176677869), (\"('but', 'i')\", 0.001435609644686613), (\"('you', 'have')\", 0.001435609644686613), (\"('and', 'the')\", 0.001419295898724265), (\"('the', 'way')\", 0.001411139025743091), (\"('tell', 'you')\", 0.0013948252797807432), (\"('all', 'of')\", 0.0013866684067995693), (\"('to', 'get')\", 0.0013866684067995693), (\"('it', 'was')\", 0.0013621977878560474), (\"('thank', 'you')\", 0.0013621977878560474), (\"('dont', 'know')\", 0.001313256549969004), (\"('one', 'of')\", 0.0012806290580443082), (\"('our', 'country')\", 0.0012643153120819603), (\"('i', 'want')\", 0.0012480015661196124), (\"('i', 'was')\", 0.0012398446931384385), (\"('do', 'it')\", 0.0012316878201572646), (\"('a', 'great')\", 0.0012235309471760906), (\"('the', 'world')\", 0.001174589709289047), (\"('with', 'the')\", 0.001158275963326699), (\"('is', 'a')\", 0.0011419622173643512), (\"('for', 'the')\", 0.0011174915984208294), (\"('and', 'then')\", 0.0011093347254396554), (\"('they', 'have')\", 0.0011093347254396554), (\"('its', 'a')\", 0.0010930209794773075), (\"('they', 'dont')\", 0.0010767072335149596)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Bigram model\")\n",
    "ngram(2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEJITH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total possible ngrams=\n",
      "2503600242967656\n",
      "\n",
      " Ngrams that actually exist=\n",
      "109775\n",
      "\n",
      " MLE for top 50 :\n",
      "\n",
      "[(\"('were', 'going', 'to')\", 0.004664085629697108), (\"('going', 'to', 'be')\", 0.0024140286950580735), (\"('a', 'lot', 'of')\", 0.002076975631974493), (\"('not', 'going', 'to')\", 0.0015850603507173765), (\"('we', 'have', 'to')\", 0.001503074470507857), (\"('going', 'to', 'have')\", 0.0011386927806877705), (\"('going', 'to', 'do')\", 0.0011204736961967661), (\"('by', 'the', 'way')\", 0.001111364153951264), (\"('one', 'of', 'the')\", 0.001065816442723753), (\"('im', 'going', 'to')\", 0.00101115918925074), (\"('i', 'want', 'to')\", 0.0009838305625142338), (\"('the', 'united', 'states')\", 0.0009018446823047142), (\"('are', 'going', 'to')\", 0.0008654065133227055), (\"('i', 'dont', 'know')\", 0.000838077886586199), (\"('and', 'you', 'know')\", 0.000838077886586199), (\"('youre', 'going', 'to')\", 0.0008289683443406969), (\"('theyre', 'going', 'to')\", 0.0008198588020951948), (\"('its', 'going', 'to')\", 0.0008016397176041904), (\"('and', 'were', 'going')\", 0.0007743110908676839), (\"('we', 'have', 'a')\", 0.0007560920063766796), (\"('going', 'to', 'win')\", 0.0007469824641311774), (\"('going', 'to', 'happen')\", 0.0007469824641311774), (\"('is', 'going', 'to')\", 0.0007378729218856752), (\"('to', 'do', 'it')\", 0.000728763379640173), (\"('i', 'dont', 'want')\", 0.0006832156684126623), (\"('you', 'look', 'at')\", 0.0006832156684126623), (\"('going', 'to', 'get')\", 0.0006741061261671601), (\"('you', 'know', 'what')\", 0.0006467774994306536), (\"('in', 'the', 'world')\", 0.0006285584149396492), (\"('going', 'to', 'make')\", 0.0005647916192211341), (\"('dont', 'want', 'to')\", 0.000555682076975632), (\"('have', 'to', 'do')\", 0.0005465725347301298), (\"('and', 'we', 'have')\", 0.0005374629924846276), (\"('ill', 'tell', 'you')\", 0.0005192439079936233), (\"('they', 'want', 'to')\", 0.0005010248235026189), (\"('we', 'are', 'going')\", 0.0004919152812571169), (\"('i', 'have', 'a')\", 0.0004919152812571169), (\"('you', 'know', 'i')\", 0.00048280573901161465), (\"('you', 'have', 'to')\", 0.00046458665452061036), (\"('take', 'care', 'of')\", 0.0004554771122751082), (\"('and', 'i', 'said')\", 0.0004554771122751082), (\"('all', 'over', 'the')\", 0.000446367570029606), (\"('and', 'i', 'think')\", 0.000446367570029606), (\"('to', 'be', 'a')\", 0.00043725802778410384), (\"('and', 'by', 'the')\", 0.00043725802778410384), (\"('i', 'have', 'to')\", 0.00043725802778410384), (\"('were', 'not', 'going')\", 0.00042814848553860167), (\"('think', 'of', 'it')\", 0.00042814848553860167), (\"('thank', 'you', 'very')\", 0.0004190389432930995), (\"('all', 'of', 'the')\", 0.0004099294010475974)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Trigram model\")\n",
    "ngram(3,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadgram model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEJITH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total possible ngrams=\n",
      "339953862591606137616\n",
      "\n",
      " Ngrams that actually exist=\n",
      "97526\n",
      "\n",
      " MLE for top 50 :\n",
      "\n",
      "[(\"('and', 'were', 'going', 'to')\", 0.0008613087791973423), (\"('were', 'going', 'to', 'do')\", 0.0006049668806267047), (\"('were', 'going', 'to', 'have')\", 0.0005844595287410537), (\"('its', 'going', 'to', 'be')\", 0.0005742058527982281), (\"('we', 'are', 'going', 'to')\", 0.0005331911490269261), (\"('not', 'going', 'to', 'happen')\", 0.0005126837971412752), (\"('were', 'going', 'to', 'win')\", 0.00048192276931279866), (\"('were', 'not', 'going', 'to')\", 0.00048192276931279866), (\"('and', 'by', 'the', 'way')\", 0.00046141541742714766), (\"('thank', 'you', 'very', 'much')\", 0.00045116174148432213), (\"('i', 'dont', 'want', 'to')\", 0.00043065438959867113), (\"('were', 'going', 'to', 'be')\", 0.00043065438959867113), (\"('were', 'going', 'to', 'make')\", 0.0004204007136558456), (\"('make', 'america', 'great', 'again')\", 0.0003998933617701946), (\"('a', 'lot', 'of', 'people')\", 0.0003486249820560671), (\"('to', 'take', 'care', 'of')\", 0.00032811763017041607), (\"('a', 'lot', 'of', 'money')\", 0.00032811763017041607), (\"('going', 'to', 'take', 'care')\", 0.0002973566023419396), (\"('im', 'not', 'going', 'to')\", 0.0002973566023419396), (\"('going', 'to', 'be', 'a')\", 0.0002973566023419396), (\"('is', 'going', 'to', 'be')\", 0.0002973566023419396), (\"('think', 'were', 'going', 'to')\", 0.00028710292639911407), (\"('have', 'a', 'lot', 'of')\", 0.00028710292639911407), (\"('we', 'have', 'to', 'do')\", 0.0002768492504562886), (\"('were', 'going', 'to', 'take')\", 0.00026659557451346307), (\"('i', 'think', 'were', 'going')\", 0.00026659557451346307), (\"('going', 'to', 'have', 'a')\", 0.00026659557451346307), (\"('take', 'care', 'of', 'our')\", 0.00026659557451346307), (\"('ill', 'tell', 'you', 'what')\", 0.00026659557451346307), (\"('were', 'going', 'to', 'get')\", 0.00026659557451346307), (\"('and', 'you', 'know', 'what')\", 0.00026659557451346307), (\"('all', 'over', 'the', 'place')\", 0.00026659557451346307), (\"('its', 'not', 'going', 'to')\", 0.0002563418985706376), (\"('not', 'going', 'to', 'be')\", 0.0002563418985706376), (\"('have', 'to', 'do', 'it')\", 0.0002563418985706376), (\"('so', 'were', 'going', 'to')\", 0.00024608822262781207), (\"('were', 'going', 'to', 'bring')\", 0.00024608822262781207), (\"('going', 'to', 'do', 'it')\", 0.00023583454668498657), (\"('we', 'have', 'to', 'be')\", 0.00023583454668498657), (\"('we', 're', 'going', 'to')\", 0.00023583454668498657), (\"('and', 'its', 'going', 'to')\", 0.00022558087074216106), (\"('i', 'just', 'want', 'to')\", 0.00022558087074216106), (\"('to', 'get', 'rid', 'of')\", 0.00021532719479933556), (\"('to', 'build', 'a', 'wall')\", 0.00021532719479933556), (\"('you', 'look', 'at', 'the')\", 0.00021532719479933556), (\"('pay', 'for', 'the', 'wall')\", 0.00021532719479933556), (\"('in', 'the', 'history', 'of')\", 0.00021532719479933556), (\"('and', 'we', 'have', 'to')\", 0.00021532719479933556), (\"('all', 'of', 'a', 'sudden')\", 0.00020507351885651006), (\"('going', 'to', 'pay', 'for')\", 0.00020507351885651006)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Quadgram model\")\n",
    "ngram(4,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Perplexity for test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n is the order of the n-gram(uni,bi,tri etc.)\n",
    "def perplexity(n):    \n",
    "    ngram_dict = {}\n",
    "    total_ngrams = 0\n",
    "    prod=1\n",
    "    uniquewords={}\n",
    "    uniquewordcount=0\n",
    "    for sentence in test_data:\n",
    "      words = nltk.word_tokenize(sentence)\n",
    "      for word in words:\n",
    "            if word not in uniquewords:\n",
    "                uniquewordcount+=1\n",
    "      ngram_list = ngrams(words, n)\n",
    "      for ngram_combination in ngram_list:\n",
    "        total_ngrams += 1\n",
    "        if ngram_combination in ngram_dict:\n",
    "            ngram_dict[ngram_combination] += 1\n",
    "        else:\n",
    "            ngram_dict[ngram_combination] = 1\n",
    "    ngram_prob_dict = {}\n",
    "    for key, val in reversed(sorted(ngram_dict.items(), key = operator.itemgetter(1))):\n",
    "      ngram_prob_dict[str(key)] = val / total_ngrams\n",
    "      prod=prod*(val**(1/len(ngram_dict)))\n",
    "    perplexity=product/total_ngrams\n",
    "    print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Unigram:\n",
      "8.143323838173156e-10\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity of Unigram:\")\n",
    "perplexity(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Bigram:\n",
      "9.025873947967283e-10\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity of Bigram:\")\n",
    "perplexity(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Trigram:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEJITH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0093963884643966e-09\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity of Trigram:\")\n",
    "perplexity(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Quadgram:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEJITH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1376241272664655e-09\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity of Quadgram:\")\n",
    "perplexity(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As we see, the perplexity increases with increase in the value of n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) Generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(n,lengthofsentence):\n",
    "#Generate using unigram\n",
    "    total_num_of_ngrams,ngram_prob_dict=ngram(n,2)\n",
    "    multi_list = np.random.multinomial(total_num_of_ngrams//lengthofsentence, [1/lengthofsentence]*lengthofsentence, size=1)\n",
    "    sentencetobeprinted = ''\n",
    "    idx = 0\n",
    "    for key, val in reversed(sorted(ngram_prob_dict.items(), key = operator.itemgetter(1))):\n",
    "      for j in multi_list[0]:\n",
    "        if idx == j:\n",
    "          sentencetobeprinted += key[2:-2]\n",
    "          sentencetobeprinted += ' '\n",
    "      idx += 1\n",
    "    print(sentencetobeprinted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Model Generation: \n",
      "\n",
      "\n",
      "deductability' \n",
      "containing' pentagon' clearsighted' differences' happiness' dan' \n",
      "asleep' weakened' weakening' abandoned' deploy' england' lightweight' \n",
      "disavow' hits' admitted' accomplished' indiana' dominant' marine' circumstance' \n",
      "paul' quite' traveled' field' piece' someone' post' mistakes' jr' \n"
     ]
    }
   ],
   "source": [
    "print(\"Unigram Model Generation: \\n\")\n",
    "for i in range(6):\n",
    "    Generator(1,4+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Model Generation: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEJITH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "than', 'he the', 'brain the', 'trigger leaving', 'and \n",
      "who', 'wants im', 'number want', 'us them', 'right the', 'rest \n",
      "for', 'office in', 'oh the', 'chinese everybody', 'wanted trump', 'administration gotten', 'to \n",
      "think', 'this smartest', 'people of', 'bad so', 'proud they', 'going wouldnt', 'even told', 'you \n",
      "deficit', 'with to', 'live from', 'mexico for', 'people wonderful', 'people talented', 'people so', 'beautiful me', 'just \n",
      "to', 'cut everybody', 'in is', 'i a', 'really because', 'our to', 'straighten country', 'that to', 'hit way', 'we \n"
     ]
    }
   ],
   "source": [
    "print(\"Bigram Model Generation: \\n\")\n",
    "for i in range(6):\n",
    "    Generator(2,4+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Model Generation: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEJITH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of', 'the', 'commentators have', 'millions', 'and a', 'special', 'person an', 'ivy', 'league \n",
      "beating', 'hillary', 'clinton i', 'saw', 'a i', 'read', 'it i', 'happen', 'to cheapest', 'thing', 'you \n",
      "well', 'have', 'to they', 'say', 'we they', 're', 'doing come', 'back', 'and them', 'and', 'we on', 'the', 'other \n",
      "impossible', 'for', 'our a', 'free', 'trader actually', 'a', 'very came', 'to', 'me so', 'many', 'others because', 'we', 'are get', 'the', 'gulf \n",
      "is', 'a', 'big look', 'forward', 'to or', 'the', 'other if', 'i', 'would theyre', 'great', 'people but', 'if', 'i because', 'they', 'dont it', 'would', 'have \n",
      "dont', 'know', 'who said', 'were', 'going is', 'a', 'very a', 'great', 'feeling you', 'know', 'with to', 'thank', 'my then', 'they', 'said have', 'great', 'respect a', 'president', 'that \n"
     ]
    }
   ],
   "source": [
    "print(\"Trigram Model Generation: \\n\")\n",
    "for i in range(6):\n",
    "    Generator(3,4+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadgram Model Generation: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEJITH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "putting', 'the', 'security', 'of with', 'deep', 'pockets', 'have indiana', 'and', 'ohio', 'whose off', 'and', 'just', 'rapidly \n",
      "wounded', 'and', 'their', 'families for', 'our', 'nations', 'loss single', 'american', 'to', 'live live', 'in', 'peace', 'and the', 'only', 'reason', 'the \n",
      "we', 'have', 'to', 'protect do', 'that', 'to', 'ford 18', 'trillion', 'in', 'debt he', 'couldnt', 'answer', 'the building', 'all', 'over', 'the and', 'frankly', 'i', 'think \n",
      "on', 'the', 'second', 'amendment the', 'tax', 'is', 'so be', 'a', 'country', 'again thats', 'what', 'were', 'going are', 'going', 'to', 'happen are', 'going', 'to', 'happen they', 'get', 'five', 'of \n",
      "country', 'in', 'the', 'world they', 'would', 'have', 'said we', 'dont', 'know', 'about we', 'have', '28', '000 couldnt', 'get', 'an', 'environmental she', 'had', 'a', 'tough think', 'we', 're', 'going killing', 'us', 'at', 'the \n",
      "allies', 'in', 'the', 'region what', 'do', 'you', 'mean couple', 'of', 'months', 'ago not', 'going', 'to', 'forget i', 'want', 'to', 'just we', 'have', 'a', 'tremendous a', 'trade', 'deficit', 'with dont', 'know', 'how', 'to friend', 'of', 'mine', 'whos \n"
     ]
    }
   ],
   "source": [
    "print(\"Quadgram Model Generation: \\n\")\n",
    "for i in range(6):\n",
    "    Generator(4,4+i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# as we see most of the sentences are unreadable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Neural Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take a list of all the characters in all the words\n",
    "characters = list(set(speeches))\n",
    "vocabularysize = len(characters)\n",
    "sequencelength = 100\n",
    "index_to_char = {index:char for index, char in enumerate(characters)}\n",
    "char_to_index = {char:index for index, char in enumerate(characters)}\n",
    "X = np.zeros((len(speeches)//sequencelength, sequencelength, vocabularysize))\n",
    "Y = np.zeros((len(speeches)//sequencelength, sequencelength, vocabularysize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(speeches)//sequencelength):\n",
    "    X_sequence = speeches[i*sequencelength:(i+1)*sequencelength]\n",
    "    X_sequence_index = [char_to_index[value] for value in X_sequence]\n",
    "    input_sequence = np.zeros((sequencelength, vocabularysize))\n",
    "    for j in range(sequencelength):\n",
    "        input_sequence[j][X_sequence_index[j]] = 1.\n",
    "    X[i] = input_sequence\n",
    "\n",
    "    Y_sequence = speeches[i*sequencelength+1:(i+1)*sequencelength+1]\n",
    "    Y_sequence_index = [char_to_index[value] for value in Y_sequence]\n",
    "    output_sequence = np.zeros((sequencelength, vocabularysize))\n",
    "    for j in range(sequencelength):\n",
    "        output_sequence[j][Y_sequence_index[j]] = 1.\n",
    "    Y[i] = output_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the model for generating the text\n",
    "def neuralgenerator(model, length):\n",
    "    ix = [np.random.randint(vocabularysize)]\n",
    "    y_char = [index_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocabularysize))\n",
    "    for i in range(length):\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(index_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(index_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the generate text model on our neural network  to generate 5 sentences\n",
    "def buildneuralnetwork(model,n):\n",
    "    epochcount = 0\n",
    "    number_of_sentences_to_be_printed = n\n",
    "    while number_of_sentences_to_be_printed > 0:\n",
    "        print('\\n')\n",
    "        number_of_sentences_to_be_printed -= 1\n",
    "        model.fit(X, Y, batch_size = 200, verbose = 1, nb_epoch=1)\n",
    "        epochcount += 1\n",
    "        neuralgenerator(model, 80)\n",
    "        if epochcount % 10 == 0:\n",
    "            model.save_weights('checkpoint_{}_epoch_{}.hdf5'.format(5, epochcount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Vanilla RNN Based Neural Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_9 (SimpleRNN)     (None, None, 500)         296500    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 92)          46092     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, None, 92)          0         \n",
      "=================================================================\n",
      "Total params: 342,592\n",
      "Trainable params: 342,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "dimensions = 500\n",
    "RNN = SimpleRNN(dimensions, input_shape=(None, len_of_vocabulary), return_sequences=True)\n",
    "denselayer = Dense(len_of_vocabulary, name='dense')\n",
    "model.add(RNN)\n",
    "model.add(denselayer)\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEJITH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "8962/8962 [==============================] - 55s 6ms/step - loss: 3.4397\n",
      "g the                                                                           \n",
      "\n",
      "Epoch 1/1\n",
      "8962/8962 [==============================] - 54s 6ms/step - loss: 2.9760\n",
      " Iou the  ao  an  ao  an  ao  an  ao  an  ao  an  ao  an  ao  an  ao  an  ao  an\n",
      "\n",
      "Epoch 1/1\n",
      "8962/8962 [==============================] - 53s 6ms/step - loss: 2.6123\n",
      "quthe wert thet thet thetheng the theng the theng the theng the theng the theng \n",
      "\n",
      "Epoch 1/1\n",
      "8962/8962 [==============================] - 53s 6ms/step - loss: 2.4056\n",
      "RERENGOTHENERENERENTHENE THE The pre the the per and the wand the wand the were \n",
      "\n",
      "Epoch 1/1\n",
      "8962/8962 [==============================] - 53s 6ms/step - loss: 2.2516\n",
      "ND THENG ING TOUNE TOENG TOUND IN The the walling to be the wand the wand the wa"
     ]
    }
   ],
   "source": [
    "buildneuralnetwork(model,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b)LSTM based neural approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, None, 500)         1186000   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, None, 500)         2002000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 92)          46092     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, None, 92)          8556      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, None, 92)          0         \n",
      "=================================================================\n",
      "Total params: 3,242,648\n",
      "Trainable params: 3,242,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "num_of_dimensions = 500\n",
    "denselayer = Dense(len_of_vocabulary, name='dense')\n",
    "model.add(LSTM(num_of_dimensions, input_shape=(None, len_of_vocabulary), return_sequences=True))\n",
    "model.add(LSTM(num_of_dimensions, return_sequences=True))\n",
    "model.add(denselayer)\n",
    "model.add(TimeDistributed(Dense(len_of_vocabulary)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEJITH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "8962/8962 [==============================] - 483s 54ms/step - loss: 3.5319\n",
      "_                                                                               \n",
      "\n",
      "Epoch 1/1\n",
      "8962/8962 [==============================] - 472s 53ms/step - loss: 3.1838\n",
      "z                                                                               \n",
      "\n",
      "Epoch 1/1\n",
      "8962/8962 [==============================] - 579s 65ms/step - loss: 3.1099\n",
      "d                                                                               \n",
      "\n",
      "Epoch 1/1\n",
      "8962/8962 [==============================] - 497s 56ms/step - loss: 2.7365\n",
      "8 oote tore tore tore tore tore tore tore tore tore tore tore tore tore tore tor\n",
      "\n",
      "Epoch 1/1\n",
      "8962/8962 [==============================] - 499s 56ms/step - loss: 2.4442\n",
      "re tore to to to to to to to to to to to to to to to to to to to to to to to to "
     ]
    }
   ],
   "source": [
    "buildneuralnetwork(model,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As we can see the sentences are not readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) Neural networks indeed do perform better than classic language models such as ngram since it doesn't suffer from the curse of dimensionality. They achieve this by representing words in a distributed way as a non linear combination of weights in a neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
