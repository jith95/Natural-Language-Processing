{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SREEJITH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, SimpleRNN, LSTM, TimeDistributed\n",
    "nltk.download('punkt')\n",
    "#opened the donald trump speech file\n",
    "speeches= open(r\"speeches.txt\", encoding=\"utf8\")\n",
    "speeches = speeches.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used the sentence tokenizer and converted everything to lowercase and then removed \"\\n\", \" \\' \" and unnecessary punctuations.\n",
    "cleanedsentences=[]\n",
    "tokenizedsentences = nltk.sent_tokenize(speeches)\n",
    "for i in tokenizedsentences:\n",
    "    i=i.lower()\n",
    "    cleansentence=re.sub(\"\\n\", ' ', i)\n",
    "    cleansentence=re.sub(\"\\’\", '', i)\n",
    "    cleansentence= \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in cleansentence]).split())\n",
    "    cleanedsentences.append(cleansentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing the dataset into 80% train and 20% test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = cleanedsentences[:int(len(cleanedsentences)*0.8)]\n",
    "test_data = cleanedsentences[int(len(cleanedsentences)*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical N gram approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general ngram function with n>=1:\n",
    "def ngram(n):\n",
    "    ngram_dict = {}\n",
    "    total_ngrams = 0\n",
    "    for sentence in train_data:\n",
    "      words = nltk.word_tokenize(sentence)\n",
    "      ngram_list = ngrams(words, n)\n",
    "      for ngram_combination in ngram_list:\n",
    "        total_ngrams += 1\n",
    "        if ngram_combination in ngram_dict:\n",
    "            ngram_dict[ngram_combination] += 1\n",
    "        else:\n",
    "            ngram_dict[ngram_combination] = 1\n",
    "    ngram_prob_dict = {}\n",
    "    for key, val in reversed(sorted(ngram_dict.items(), key = operator.itemgetter(1))):\n",
    "      ngram_prob_dict[str(key)] = val / total_ngrams\n",
    "    print (\"Top 50 probability values in the sorted order:\\n\\n\")\n",
    "    print(list(ngram_prob_dict.items())[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 probability values in the sorted order:\n",
      "[(\"('the',)\", 0.03439235267258775), (\"('to',)\", 0.03255858483201508), (\"('and',)\", 0.03210198400424197), (\"('i',)\", 0.02859646797166129), (\"('a',)\", 0.02132767737469253), (\"('you',)\", 0.01840395917104856), (\"('of',)\", 0.01711516651201155), (\"('we',)\", 0.015892654618296437), (\"('it',)\", 0.014353467956932231), (\"('that',)\", 0.013653837656312139), (\"('have',)\", 0.013602285949950658), (\"('they',)\", 0.012821645825048238), (\"('going',)\", 0.012320857820393855), (\"('in',)\", 0.011864256992620741), (\"('so',)\", 0.009109922967021638), (\"('is',)\", 0.009036277672219522), (\"('but',)\", 0.00829982472419837), (\"('know',)\", 0.007916869191227372), (\"('were',)\", 0.007835859366945046), (\"('people',)\", 0.0077622140721429304), (\"('–',)\", 0.007526549128776163), (\"('its',)\", 0.00741608118657299), (\"('be',)\", 0.007150958125285376), (\"('are',)\", 0.006318766294021475), (\"('for',)\", 0.006208298351818302), (\"('not',)\", 0.006038914173773438), (\"('this',)\", 0.00600209152637238), (\"('do',)\", 0.005965268878971323), (\"('our',)\", 0.005891623584169207), (\"('with',)\", 0.005781155641966035), (\"('was',)\", 0.0057590620535254), (\"('he',)\", 0.005744332994564977), (\"('dont',)\", 0.0056265005228815935), (\"('all',)\", 0.005044702693944884), (\"('im',)\", 0.004912141163301077), (\"('what',)\", 0.004860589456939596), (\"('because',)\", 0.004632289043053039), (\"('want',)\", 0.004521821100849866), (\"('very',)\", 0.004514456571369656), (\"('me',)\", 0.004411353158646694), (\"('said',)\", 0.004396624099686271), (\"('theyre',)\", 0.004374530511245637), (\"('on',)\", 0.004315614275403944), (\"('about',)\", 0.003800097211789139), (\"('get',)\", 0.003800097211789139), (\"('great',)\", 0.003800097211789139), (\"('think',)\", 0.0037264519169870236), (\"('like',)\", 0.003719087387506812), (\"('if',)\", 0.0036233485042640626), (\"('one',)\", 0.003431870737778563)]\n"
     ]
    }
   ],
   "source": [
    "#unigram\n",
    "ngram(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEJITH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 probability values in the sorted order:\n",
      "\n",
      "\n",
      "[(\"('going', 'to')\", 0.012341348820516167), (\"('you', 'know')\", 0.005522203008254755), (\"('we', 'have')\", 0.004510750758589187), (\"('were', 'going')\", 0.004274201442135143), (\"('and', 'i')\", 0.00391529903096349), (\"('to', 'be')\", 0.003907142157982316), (\"('of', 'the')\", 0.0038255734281705763), (\"('want', 'to')\", 0.0032790629384319226), (\"('have', 'to')\", 0.0030425136219778787), (\"('to', 'do')\", 0.0026428268459003554), (\"('in', 'the')\", 0.0025857287350321383), (\"('i', 'dont')\", 0.0025775718620509643), (\"('a', 'lot')\", 0.002479689386276877), (\"('i', 'think')\", 0.0022757675617475284), (\"('have', 'a')\", 0.0021534144670299193), (\"('i', 'mean')\", 0.0021452575940487453), (\"('and', 'they')\", 0.002055531991255832), (\"('i', 'have')\", 0.001990277007406441), (\"('i', 'said')\", 0.001941335769519397), (\"('lot', 'of')\", 0.0018679239126888317), (\"('and', 'we')\", 0.0017129433260465268), (\"('and', 'you')\", 0.0016232177232536134), (\"('by', 'the')\", 0.0015090215015171784), (\"('to', 'have')\", 0.0014763940095924826), (\"('not', 'going')\", 0.0014600802636301348), (\"('look', 'at')\", 0.0014437665176677869), (\"('but', 'i')\", 0.001435609644686613), (\"('you', 'have')\", 0.001435609644686613), (\"('and', 'the')\", 0.001419295898724265), (\"('the', 'way')\", 0.001411139025743091), (\"('tell', 'you')\", 0.0013948252797807432), (\"('all', 'of')\", 0.0013866684067995693), (\"('to', 'get')\", 0.0013866684067995693), (\"('it', 'was')\", 0.0013621977878560474), (\"('thank', 'you')\", 0.0013621977878560474), (\"('dont', 'know')\", 0.001313256549969004), (\"('one', 'of')\", 0.0012806290580443082), (\"('our', 'country')\", 0.0012643153120819603), (\"('i', 'want')\", 0.0012480015661196124), (\"('i', 'was')\", 0.0012398446931384385), (\"('do', 'it')\", 0.0012316878201572646), (\"('a', 'great')\", 0.0012235309471760906), (\"('the', 'world')\", 0.001174589709289047), (\"('with', 'the')\", 0.001158275963326699), (\"('is', 'a')\", 0.0011419622173643512), (\"('for', 'the')\", 0.0011174915984208294), (\"('and', 'then')\", 0.0011093347254396554), (\"('they', 'have')\", 0.0011093347254396554), (\"('its', 'a')\", 0.0010930209794773075), (\"('they', 'dont')\", 0.0010767072335149596)]\n"
     ]
    }
   ],
   "source": [
    "#bigram\n",
    "ngram(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEJITH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 probability values in the sorted order:\n",
      "\n",
      "\n",
      "[(\"('were', 'going', 'to')\", 0.004664085629697108), (\"('going', 'to', 'be')\", 0.0024140286950580735), (\"('a', 'lot', 'of')\", 0.002076975631974493), (\"('not', 'going', 'to')\", 0.0015850603507173765), (\"('we', 'have', 'to')\", 0.001503074470507857), (\"('going', 'to', 'have')\", 0.0011386927806877705), (\"('going', 'to', 'do')\", 0.0011204736961967661), (\"('by', 'the', 'way')\", 0.001111364153951264), (\"('one', 'of', 'the')\", 0.001065816442723753), (\"('im', 'going', 'to')\", 0.00101115918925074), (\"('i', 'want', 'to')\", 0.0009838305625142338), (\"('the', 'united', 'states')\", 0.0009018446823047142), (\"('are', 'going', 'to')\", 0.0008654065133227055), (\"('i', 'dont', 'know')\", 0.000838077886586199), (\"('and', 'you', 'know')\", 0.000838077886586199), (\"('youre', 'going', 'to')\", 0.0008289683443406969), (\"('theyre', 'going', 'to')\", 0.0008198588020951948), (\"('its', 'going', 'to')\", 0.0008016397176041904), (\"('and', 'were', 'going')\", 0.0007743110908676839), (\"('we', 'have', 'a')\", 0.0007560920063766796), (\"('going', 'to', 'win')\", 0.0007469824641311774), (\"('going', 'to', 'happen')\", 0.0007469824641311774), (\"('is', 'going', 'to')\", 0.0007378729218856752), (\"('to', 'do', 'it')\", 0.000728763379640173), (\"('i', 'dont', 'want')\", 0.0006832156684126623), (\"('you', 'look', 'at')\", 0.0006832156684126623), (\"('going', 'to', 'get')\", 0.0006741061261671601), (\"('you', 'know', 'what')\", 0.0006467774994306536), (\"('in', 'the', 'world')\", 0.0006285584149396492), (\"('going', 'to', 'make')\", 0.0005647916192211341), (\"('dont', 'want', 'to')\", 0.000555682076975632), (\"('have', 'to', 'do')\", 0.0005465725347301298), (\"('and', 'we', 'have')\", 0.0005374629924846276), (\"('ill', 'tell', 'you')\", 0.0005192439079936233), (\"('they', 'want', 'to')\", 0.0005010248235026189), (\"('we', 'are', 'going')\", 0.0004919152812571169), (\"('i', 'have', 'a')\", 0.0004919152812571169), (\"('you', 'know', 'i')\", 0.00048280573901161465), (\"('you', 'have', 'to')\", 0.00046458665452061036), (\"('take', 'care', 'of')\", 0.0004554771122751082), (\"('and', 'i', 'said')\", 0.0004554771122751082), (\"('all', 'over', 'the')\", 0.000446367570029606), (\"('and', 'i', 'think')\", 0.000446367570029606), (\"('to', 'be', 'a')\", 0.00043725802778410384), (\"('and', 'by', 'the')\", 0.00043725802778410384), (\"('i', 'have', 'to')\", 0.00043725802778410384), (\"('were', 'not', 'going')\", 0.00042814848553860167), (\"('think', 'of', 'it')\", 0.00042814848553860167), (\"('thank', 'you', 'very')\", 0.0004190389432930995), (\"('all', 'of', 'the')\", 0.0004099294010475974)]\n"
     ]
    }
   ],
   "source": [
    "#trigram\n",
    "ngram(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEJITH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 probability values in the sorted order:\n",
      "\n",
      "\n",
      "[(\"('and', 'were', 'going', 'to')\", 0.0008613087791973423), (\"('were', 'going', 'to', 'do')\", 0.0006049668806267047), (\"('were', 'going', 'to', 'have')\", 0.0005844595287410537), (\"('its', 'going', 'to', 'be')\", 0.0005742058527982281), (\"('we', 'are', 'going', 'to')\", 0.0005331911490269261), (\"('not', 'going', 'to', 'happen')\", 0.0005126837971412752), (\"('were', 'going', 'to', 'win')\", 0.00048192276931279866), (\"('were', 'not', 'going', 'to')\", 0.00048192276931279866), (\"('and', 'by', 'the', 'way')\", 0.00046141541742714766), (\"('thank', 'you', 'very', 'much')\", 0.00045116174148432213), (\"('i', 'dont', 'want', 'to')\", 0.00043065438959867113), (\"('were', 'going', 'to', 'be')\", 0.00043065438959867113), (\"('were', 'going', 'to', 'make')\", 0.0004204007136558456), (\"('make', 'america', 'great', 'again')\", 0.0003998933617701946), (\"('a', 'lot', 'of', 'people')\", 0.0003486249820560671), (\"('to', 'take', 'care', 'of')\", 0.00032811763017041607), (\"('a', 'lot', 'of', 'money')\", 0.00032811763017041607), (\"('going', 'to', 'take', 'care')\", 0.0002973566023419396), (\"('im', 'not', 'going', 'to')\", 0.0002973566023419396), (\"('going', 'to', 'be', 'a')\", 0.0002973566023419396), (\"('is', 'going', 'to', 'be')\", 0.0002973566023419396), (\"('think', 'were', 'going', 'to')\", 0.00028710292639911407), (\"('have', 'a', 'lot', 'of')\", 0.00028710292639911407), (\"('we', 'have', 'to', 'do')\", 0.0002768492504562886), (\"('were', 'going', 'to', 'take')\", 0.00026659557451346307), (\"('i', 'think', 'were', 'going')\", 0.00026659557451346307), (\"('going', 'to', 'have', 'a')\", 0.00026659557451346307), (\"('take', 'care', 'of', 'our')\", 0.00026659557451346307), (\"('ill', 'tell', 'you', 'what')\", 0.00026659557451346307), (\"('were', 'going', 'to', 'get')\", 0.00026659557451346307), (\"('and', 'you', 'know', 'what')\", 0.00026659557451346307), (\"('all', 'over', 'the', 'place')\", 0.00026659557451346307), (\"('its', 'not', 'going', 'to')\", 0.0002563418985706376), (\"('not', 'going', 'to', 'be')\", 0.0002563418985706376), (\"('have', 'to', 'do', 'it')\", 0.0002563418985706376), (\"('so', 'were', 'going', 'to')\", 0.00024608822262781207), (\"('were', 'going', 'to', 'bring')\", 0.00024608822262781207), (\"('going', 'to', 'do', 'it')\", 0.00023583454668498657), (\"('we', 'have', 'to', 'be')\", 0.00023583454668498657), (\"('we', 're', 'going', 'to')\", 0.00023583454668498657), (\"('and', 'its', 'going', 'to')\", 0.00022558087074216106), (\"('i', 'just', 'want', 'to')\", 0.00022558087074216106), (\"('to', 'get', 'rid', 'of')\", 0.00021532719479933556), (\"('to', 'build', 'a', 'wall')\", 0.00021532719479933556), (\"('you', 'look', 'at', 'the')\", 0.00021532719479933556), (\"('pay', 'for', 'the', 'wall')\", 0.00021532719479933556), (\"('in', 'the', 'history', 'of')\", 0.00021532719479933556), (\"('and', 'we', 'have', 'to')\", 0.00021532719479933556), (\"('all', 'of', 'a', 'sudden')\", 0.00020507351885651006), (\"('going', 'to', 'pay', 'for')\", 0.00020507351885651006)]\n"
     ]
    }
   ],
   "source": [
    "#quadgram\n",
    "ngram(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
