{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjj-9snwgL88"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import scikitplot as skplt\n",
    "import string\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout,BatchNormalization, Activation,SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB as mnb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removed stopwords,punctuation and appended the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gxBKqNrxdl4z"
   },
   "outputs": [],
   "source": [
    "def stopwords_remove(text):\n",
    "    stop = stopwords.words('english')\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "\n",
    "    return \" \".join(text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def append(f)\n",
    "    sent=[]\n",
    "    cat=[]\n",
    "    final=''\n",
    "    i=0\n",
    "    for x in f:\n",
    "      if len(x)>1 and 'meta' == x.split()[0] and ('negative' == x.split()[-1]):\n",
    "        if(i!=0):\n",
    "          sent.append(final)\n",
    "          final=''\n",
    "\n",
    "        cat.append('negative')\n",
    "      elif len(x)>1 and 'meta' == x.split()[0] and ( 'positive' == x.split()[-1]):\n",
    "        if(i!=0):\n",
    "          sent.append(final)\n",
    "          final=''\n",
    "        cat.append('positive')\n",
    "      elif len(x)>1 and 'meta' == x.split()[0]and ( 'neutral' == x.split()[-1]):\n",
    "        if(i!=0):\n",
    "          sent.append(final)\n",
    "          final=''\n",
    "        cat.append('neutral')\n",
    "\n",
    "      elif len(x)>1:\n",
    "        final=final+' ' + x.split()[0]\n",
    "      i=i+1 \n",
    "    sent.append(final)\n",
    "    \n",
    "f = open(\"train.txt\", \"r\")\n",
    "append(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# did the same procedure for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pAnwgweAt95I"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['sent','cat'])\n",
    "df['sent']=sent\n",
    "df['cat']=cat\n",
    "\n",
    "f = open(\"test.txt\", \"r\")\n",
    "append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JtQuQm01uirj"
   },
   "outputs": [],
   "source": [
    "tdf=pd.DataFrame(columns=['sent','cat'])\n",
    "tdf['sent']=sent\n",
    "tdf['cat']=cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "8r3fWGe9Uxev",
    "outputId": "1e35c269-e32c-49cd-d724-4f3a676d6ede"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "0Q3DUp7GGyTd",
    "outputId": "3b292e73-4ba2-462c-cadf-4c6393395370"
   },
   "outputs": [],
   "source": [
    "\n",
    "df['sent']=df['sent'].str.lower()\n",
    "tdf['sent']=tdf['sent'].str.lower()\n",
    "df['sent'] = df['sent'].apply(remove_punctuation)\n",
    "tdf['sent']=tdf['sent'].str.lower()\n",
    "tdf['sent'] = tdf['sent'].apply(remove_punctuation)\n",
    "df['sent'] = df['sent'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "tdf['sent'] = tdf['sent'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "11X6M5TpjDN2"
   },
   "outputs": [],
   "source": [
    "X_train, X_test11, y_train, y_test11 = train_test_split(df['sent'], df['cat'], test_size=1)\n",
    "X_test, X_cv, y_test, y_cv = train_test_split(tdf['sent'], tdf['cat'], test_size=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dimensions for my neural language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fAHAMMbqq9OA",
    "outputId": "3c036fee-f589-4c78-d7f9-74f1757485e4"
   },
   "outputs": [],
   "source": [
    "# list of most frequent words\n",
    "MAX_NB_WORDS = 12000\n",
    "# Sequence length\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "# Embedding dimensions\n",
    "EMBEDDING_DIM = 1000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['sent'].values)\n",
    "word_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PnJnNUyerAts",
    "outputId": "d8fadc3a-3d3d-4050-a244-4664fc78e25b"
   },
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(df['sent'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sNcvYBlmuCE_",
    "outputId": "7a10621c-7057-4fcf-d655-b2c1a0d58eb9"
   },
   "outputs": [],
   "source": [
    "X1 = tokenizer.texts_to_sequences(tdf['sent'].values)\n",
    "X1 = pad_sequences(X1, maxlen=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoded it into a one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmbaQA_RrDEr"
   },
   "outputs": [],
   "source": [
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "arry=np.array(df['cat']).reshape(-1,1)\n",
    "enc.fit(arry)\n",
    "Y=enc.transform(arry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pg4L7xsnuSv7"
   },
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "arry=np.array(tdf['cat']).reshape(-1,1)\n",
    "enc.fit(arry)\n",
    "Y1=enc.transform(arry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "smnleOrvrGZ4",
    "outputId": "a4197123-4170-4277-b045-a6921db62b95"
   },
   "outputs": [],
   "source": [
    "X_train, X_test111, Y_train, Y_test11 = train_test_split(X,Y, test_size =1)\n",
    "X_test, X_cv, y_test, y_cv = train_test_split(X1, Y1, test_size=0.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "colab_type": "code",
    "id": "3oHT1PI_rLsD",
    "outputId": "f3f64205-bc12-4ccb-84bc-723d72f74737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 300, 800)          8000000   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 300, 800)          0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 100)               360400    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 8,360,703\n",
      "Trainable params: 8,360,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13919 samples, validate on 1211 samples\n",
      "Epoch 1/5\n",
      "13919/13919 [==============================] - 32s 2ms/step - loss: 1.0332 - acc: 0.4454 - val_loss: 0.8857 - val_acc: 0.5632\n",
      "Epoch 2/5\n",
      "13919/13919 [==============================] - 28s 2ms/step - loss: 0.8380 - acc: 0.6046 - val_loss: 0.8151 - val_acc: 0.6235\n",
      "Epoch 3/5\n",
      "13919/13919 [==============================] - 28s 2ms/step - loss: 0.6964 - acc: 0.7013 - val_loss: 0.8099 - val_acc: 0.6292\n",
      "Epoch 4/5\n",
      "13919/13919 [==============================] - 28s 2ms/step - loss: 0.5725 - acc: 0.7734 - val_loss: 0.8733 - val_acc: 0.6185\n",
      "Epoch 5/5\n",
      "13919/13919 [==============================] - 28s 2ms/step - loss: 0.4750 - acc: 0.8183 - val_loss: 0.9611 - val_acc: 0.6111\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(Dropout(0.36))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 700\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.08)#callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "miLFAwimrUqv",
    "outputId": "2efbec7b-0ea4-4807-f071-5917f0448f68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15130/15130 [==============================] - 127s 8ms/step\n",
      "Test set\n",
      "  Loss: 0.416\n",
      "  Accuracy: 0.855\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_train,Y_train)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCTiLprzQeYI"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKdSweykf9tv"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "w_IY98aYKbw-",
    "outputId": "31db9aa6-c7bc-499a-a01f-0ae4e269a3b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682/1682 [==============================] - 15s 9ms/step\n",
      "Test set\n",
      "  Loss: 1.198\n",
      "  Accuracy: 0.534\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "QVZ8Os42cvrK",
    "outputId": "81209622-89a4-493d-ffda-d99152abafd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.56      0.56      0.56       486\n",
      "     Neutral       0.44      0.52      0.47       573\n",
      "    Negative       0.63      0.53      0.58       623\n",
      "\n",
      "    accuracy                           0.53      1682\n",
      "   macro avg       0.54      0.54      0.54      1682\n",
      "weighted avg       0.55      0.53      0.54      1682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newy_p=enc.inverse_transform(y_test)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(newy_p)\n",
    "newy=model.predict_classes(X_test)\n",
    "clas=le.inverse_transform(newy)\n",
    "clas1=newy_p.flatten()\n",
    "target_names = ['Positive', 'Neutral', 'Negative']\n",
    "print(classification_report(clas,clas1, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLPAssignment3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
